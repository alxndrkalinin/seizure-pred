{
 "metadata": {
  "name": "",
  "signature": "sha256:a08212b00bece1b39d324c179170503a236936477c1821ae7ee98b2f0ebabc34"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from __future__ import division\n",
      "import os, sys, glob, pickle, itertools\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy import stats, fft\n",
      "from scipy.signal import butter, lfilter\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import cross_validation as cv\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import precision_recall_fscore_support "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 221
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "healthyDir = 'healthy'\n",
      "epilepticDir = 'epileptic'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def readData(dirName):\n",
      "    pwd = os.path.dirname(os.path.realpath(dirName))\n",
      "\n",
      "    allFiles = glob.glob(pwd + '/' + dirName + '/*.txt')\n",
      "    frames = []\n",
      "\n",
      "    for fileName in allFiles:\n",
      "        series = pd.read_csv(fileName, header=None, sep='\\n', index_col=False)\n",
      "        series = series.transpose()\n",
      "        frames.append(series)\n",
      "    \n",
      "    df = pd.concat(frames)\n",
      "    df = df.reset_index(drop = 1)\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 223
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "healthyDf = readData(healthyDir)\n",
      "epilepticDf = readData(epilepticDir)\n",
      "print(healthyDf.shape)\n",
      "print(epilepticDf.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(100, 4097)\n",
        "(100, 4097)\n"
       ]
      }
     ],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
      "    nyq = 0.5 * fs\n",
      "    low = lowcut / nyq\n",
      "    high = highcut / nyq\n",
      "    b, a = butter(order, [low, high], btype = 'band')\n",
      "    return b, a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Prepare 8 overlapping log-spaced bandpass filters between 0.53 and 40Hz\n",
      "filters = []\n",
      "fs = 173.61\n",
      "for i in range(8):\n",
      "    x = np.logspace(np.log10(0.53), np.log10(40), 10)\n",
      "    filters.append(butter_bandpass(x[i], np.floor(x[i + 2]), fs, 3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 226
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create combinatorial filter sets up to size 4. Output is global idexing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# List of all possible combinations of 4 or fewer filters\n",
      "wis = list(itertools.chain(itertools.combinations(range(8), 1),\n",
      "                           itertools.combinations(range(8), 2),\n",
      "                           itertools.combinations(range(8), 3),\n",
      "                           itertools.combinations(range(8), 4)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 227
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_features(signal):\n",
      "    feat = []\n",
      "    for b, a in filters:\n",
      "        f = lfilter(b, a, signal)\n",
      "        covar = np.cov(f)\n",
      "        ampl = np.sum(np.absolute(f)) / np.sum(np.absolute(signal).sum())\n",
      "        power = np.sum(np.power(f, 2)) / np.sum(np.power(signal, 2).sum())\n",
      "        feat = np.append(feat, [covar, ampl, power])\n",
      "    return pd.Series(feat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "healthyLabels = np.zeros(healthyDf.shape[0], dtype=np.int)\n",
      "epilepticLables = np.ones(epilepticDf.shape[0], dtype=np.int)\n",
      "\n",
      "labels = np.append(healthyLabels, epilepticLables)\n",
      "\n",
      "healthyFeat = healthyDf.apply(extract_features, axis=1)\n",
      "epilepticFeat = epilepticDf.apply(extract_features, axis=1)\n",
      "\n",
      "features = healthyFeat.append(epilepticFeat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getFeaturesForFilterSet (features, filterSet):\n",
      "    return pd.concat([features.iloc[:, 3 * filter : 3 * (filter + 1)] for filter in filterSet], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "auc_scores = []\n",
      "ig_scores = []\n",
      "rfs_scores = []\n",
      "\n",
      "def binFreq(arr):\n",
      "    nonZeroFreq = np.count_nonzero(arr) / len(arr)\n",
      "    return [nonZeroFreq, 1 - nonZeroFreq]\n",
      "\n",
      "for w in wis:\n",
      "    \n",
      "    # get features for current set of filters using global indices\n",
      "    featureSet = getFeaturesForFilterSet(features, w)\n",
      "    \n",
      "    y = labels\n",
      "    X = featureSet.values\n",
      "\n",
      "    ps = []\n",
      "    fs = []\n",
      "    rfs = []\n",
      "    for train, test in cv.ShuffleSplit(X.shape[0], random_state=42):\n",
      "        X_train = X[train]\n",
      "        X_test = X[test]\n",
      "        y_train = y[train]\n",
      "        y_test = y[test]\n",
      "\n",
      "        clf = SGDClassifier(loss='log', penalty='l1', alpha=0.0001)    \n",
      "        clf.fit(X_train, y_train)     \n",
      "        y_pred = clf.predict(X_test)\n",
      "\n",
      "        ras = roc_auc_score(y_test, y_pred)\n",
      "        igs = stats.entropy(binFreq(y_pred), binFreq(y_test))\n",
      "        ps.append(ras)\n",
      "        fs.append(igs)\n",
      "\n",
      "        \n",
      "    ps = np.array(ps)\n",
      "    fs = np.array(fs)\n",
      "    rfs = np.array(rfs)\n",
      "    auc_scores.append(ps.mean())\n",
      "    ig_scores.append(fs.mean())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 231
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Select 5 best filter sets from each selection scheme\n",
      "best_auc = sorted(zip(auc_scores, wis))[-5:]\n",
      "best_ig = sorted(zip(ig_scores, wis))[:5]\n",
      "\n",
      "print(best_auc)\n",
      "print(best_ig)\n",
      "\n",
      "best_auc_ind = list(x[1] for x in best_auc)\n",
      "best_ig_ind = list(x[1] for x in best_ig)\n",
      "\n",
      "# choose filter sets that include filters from best AUC list\n",
      "elems = set(itertools.chain.from_iterable(best_auc_ind))\n",
      "best_filters = [tup for tup in best_ig_ind if elems.issuperset(tup)]\n",
      "# best_filters = best_auc_ind + best_ig_ind\n",
      "# best_filters = best_ig_ind\n",
      "print(best_filters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0.58969336219336221, (0, 1, 2, 3)), (0.59023809523809523, (0, 1, 6)), (0.610669191919192, (0, 3)), (0.61897907647907657, (0, 2, 3)), (0.62044372294372307, (0, 1, 3))]\n",
        "[(0.33872509586235128, (0, 2, 3, 5)), (0.352665514604522, (0, 3)), (0.35804412756429993, (0, 1, 3)), (0.37052472784659302, (0, 2, 3)), (0.39300911656660331, (0, 2, 3, 4))]\n",
        "[(0, 3), (0, 1, 3), (0, 2, 3)]\n"
       ]
      }
     ],
     "prompt_number": 232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifiers = [\n",
      "    LinearSVC(dual=False),\n",
      "    RandomForestClassifier(n_estimators=200, n_jobs=3),\n",
      "    GradientBoostingClassifier(n_estimators=200, warm_start=False)\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 233
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filter_best_auc = []\n",
      "\n",
      "for feat_set in best_filters:\n",
      "    print('\\nFILTERS:' + str(feat_set))\n",
      "    \n",
      "    featureSet = getFeaturesForFilterSet(features, feat_set)\n",
      "    \n",
      "    y = labels\n",
      "    X = featureSet.values\n",
      "    \n",
      "    clfs_auc = []\n",
      "\n",
      "    for clf in classifiers:\n",
      "        \n",
      "#         print('Classifier: ' + str(clf))\n",
      "\n",
      "        aucs = []\n",
      "    \n",
      "        for train, test in cv.KFold(X.shape[0], n_folds=10, shuffle=True, random_state=24):\n",
      "            X_train = X[train]\n",
      "            X_test = X[test]\n",
      "            y_train = y[train]\n",
      "            y_test = y[test]\n",
      "            \n",
      "            clf.fit(X_train, y_train)\n",
      "            y_pred = clf.predict(X_test)\n",
      "\n",
      "            auc = roc_auc_score(y_test, y_pred)\n",
      "            aucs.append(auc)\n",
      "\n",
      "        avg_auc = np.mean(aucs)\n",
      "        print(avg_auc)\n",
      "        clfs_auc.append(avg_auc)\n",
      "\n",
      "    best_auc_clf = sorted(zip(clfs_auc, classifiers))[-1:]\n",
      "    print(best_auc_clf)\n",
      "    filter_best_auc.append(best_auc_clf[0])\n",
      "    \n",
      "best_filter = sorted(zip(filter_best_auc, best_filters))[-1:]\n",
      "print(best_filter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "FILTERS:(0, 3)\n",
        "Classifier: LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0)\n",
        "0.745833333333\n",
        "Classifier: RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=200, n_jobs=3,\n",
        "            oob_score=False, random_state=None, verbose=0)\n",
        "1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Classifier: GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
        "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
        "              min_samples_leaf=1, min_samples_split=2, n_estimators=200,\n",
        "              random_state=None, subsample=1.0, verbose=0,\n",
        "              warm_start=False)\n",
        "1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(1.0, RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=200, n_jobs=3,\n",
        "            oob_score=False, random_state=None, verbose=0))]\n",
        "\n",
        "FILTERS:(0, 1, 3)\n",
        "Classifier: LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0)\n",
        "0.820416666667\n",
        "Classifier: RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=200, n_jobs=3,\n",
        "            oob_score=False, random_state=None, verbose=0)\n",
        "0.995"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Classifier: GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
        "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
        "              min_samples_leaf=1, min_samples_split=2, n_estimators=200,\n",
        "              random_state=None, subsample=1.0, verbose=0,\n",
        "              warm_start=False)\n",
        "1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(1.0, GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
        "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
        "              min_samples_leaf=1, min_samples_split=2, n_estimators=200,\n",
        "              random_state=None, subsample=1.0, verbose=0,\n",
        "              warm_start=False))]\n",
        "\n",
        "FILTERS:(0, 2, 3)\n",
        "Classifier: LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0)\n",
        "0.840833333333\n",
        "Classifier: RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=200, n_jobs=3,\n",
        "            oob_score=False, random_state=None, verbose=0)\n",
        "0.995"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Classifier: GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
        "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
        "              min_samples_leaf=1, min_samples_split=2, n_estimators=200,\n",
        "              random_state=None, subsample=1.0, verbose=0,\n",
        "              warm_start=False)\n",
        "1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(1.0, GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
        "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
        "              min_samples_leaf=1, min_samples_split=2, n_estimators=200,\n",
        "              random_state=None, subsample=1.0, verbose=0,\n",
        "              warm_start=False))]\n",
        "[((1.0, RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=200, n_jobs=3,\n",
        "            oob_score=False, random_state=None, verbose=0)), (0, 3))]\n"
       ]
      }
     ],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Best filter set: ' + str(sorted(zip(filter_best_auc, best_filters))[-1:][0][1])\n",
      "print 'Best classifier: ' + str(sorted(zip(filter_best_auc, best_filters))[-1:][0][0][1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best filter set: (0, 3)\n",
        "Best classifier: RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
        "            min_samples_split=2, n_estimators=200, n_jobs=3,\n",
        "            oob_score=False, random_state=None, verbose=0)\n"
       ]
      }
     ],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accs = []\n",
      "aucs = []\n",
      "cm = np.zeros((2,2))\n",
      "\n",
      "featureSet = getFeaturesForFilterSet(features, best_filter[0][1])\n",
      "X = featureSet.iloc[:, 3].values\n",
      "\n",
      "if len(X.shape) < 2:\n",
      "    X.shape += (1,)\n",
      "\n",
      "clf = best_filter[0][0][1]\n",
      "\n",
      "for train, test in cv.KFold(X.shape[0], n_folds=10, shuffle=True):\n",
      "    X_train = X[train]\n",
      "    X_test = X[test]\n",
      "    y_train = y[train]\n",
      "    y_test = y[test]\n",
      "\n",
      "    clf.fit(X_train, y_train)\n",
      "    y_pred = clf.predict(X_test)\n",
      "\n",
      "    # TODO: add combinatorial feature discrimination\n",
      "    \n",
      "    acc = accuracy_score(y_test, y_pred)\n",
      "    auc = roc_auc_score(y_test, y_pred)\n",
      "    cm = np.add(cm, confusion_matrix(y_test, y_pred))\n",
      "    accs.append(acc)\n",
      "    aucs.append(auc)\n",
      "            \n",
      "# averaging classification metrics over\n",
      "print 'Avg accuracy: {0:.2f} +/- {1:.2f}'.format(np.mean(accs), np.std(accs))\n",
      "print 'Avg AUC ROC: {0:.2f} +/- {1:.2f}'.format(np.mean(aucs), np.std(aucs))\n",
      "print 'Avg confusion matrix:'\n",
      "print(np.divide(cm, 10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Avg accuracy: 1.00 +/- 0.00\n",
        "Avg AUC ROC: 1.00 +/- 0.00\n",
        "Avg confusion matrix:\n",
        "[[ 10.   0.]\n",
        " [  0.  10.]]\n"
       ]
      }
     ],
     "prompt_number": 236
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 236
    }
   ],
   "metadata": {}
  }
 ]
}